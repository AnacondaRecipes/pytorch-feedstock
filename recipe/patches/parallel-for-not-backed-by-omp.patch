From 2350f7f9f8f0434fb83e20d0d7026ed654900a70 Mon Sep 17 00:00:00 2001
From: Nikita Shulga <nikita.shulga@gmail.com>
Date: Sat, 25 Sep 2021 07:34:47 -0700
Subject: [PATCH] MaxUnpooling: parallel_for not always backed by OMP

Use atomic optional instead of omp critical pragma inside max_unpooling
kernels

Using any OpenMP pragma in `at::parallel_for` body is wrong, as it can
be implemented using native treading algorithms such as ptrheads or
openmp

`c10::optional` sounds like a much better approach to pair of
`has_error` and `error_index` variables

It also fixes https://github.com/pytorch/pytorch/issues/65578
---
 aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp | 58 +++++++-------------
 1 file changed, 20 insertions(+), 38 deletions(-)

Index: pytorch/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
===================================================================
--- pytorch.orig/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
+++ pytorch/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
@@ -9,6 +9,8 @@
 
 #include <c10/util/Optional.h>
 
+#include <c10/util/Optional.h>
+
 namespace at { namespace native {
 
 namespace {
