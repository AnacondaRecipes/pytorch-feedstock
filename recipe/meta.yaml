{% set version = "2.3.0" %}
# Set the RC number to build release candidates. Set to None otherwise
{% set rc = 2 %}
{% set sha256 = "e12d18c3dbb12d7ae2f61f5ab9a21023e3dd179d67ed87279ef96600b9ac08c5" %}
{% set build_number = 0 %}

package:
  name: pytorch-select
  version: {{ version }}

source:
{% if rc != None %}
  git_url: https://github.com/pytorch/pytorch.git
  git_rev: v{{ version }}-rc{{ rc }}
{% else %}
  # The "pytorch-v" tarballs contain submodules; the "pytorch-" ones don't.
  url: https://github.com/pytorch/pytorch/releases/download/v{{ version }}/pytorch-v{{ version }}.tar.gz
{% endif %}
  sha256: {{ sha256 }}
  patches:
    - patches/0001-windows-FindMKL-add-library-suffix.patch  # [win]
    - patches/0002-swap-openmp-search-precedence.patch      # [blas_impl == "mkl"]
    - patches/0003-continue-tests-on-failure.patch

build:
  # Use a build number difference to ensure that the GPU
  # variant is slightly preferred by conda's solver, so that it's preferentially
  # installed where the platform supports it.
  number: {{ build_number + 100 }}  # [gpu_variant != "cpu"]
  number: {{ build_number }}        # [gpu_variant == "cpu"]
  skip: True  # [py<38]
  # Dropping ppc because of various build issues
  skip: True  # [(linux and ppc64le)]

{% if rc %}
requirements:
  build:
    - git                             # [unix]
{% endif %}

outputs:
  # The pytorch-cpu and pytorch-gpu metapackages are packages which the user can use to get the
  # corresponding pytorch variant. If they install the pytorch package, it will give them the
  # GPU variant if their platform supports it and the CPU variant otherwise.
  - name: pytorch-{{ "cpu" if gpu_variant == "cpu" else "gpu" }}
    build:
      skip: True  # [py<38]
    requirements:
      run:
        - pytorch ={{ version }}={{ "cpu" if gpu_variant == "cpu" else "gpu" }}* # NB pinning exact=True will also pin to the python version

  - name: pytorch
    script: build_pytorch.sh # [not win]
    script: bld_pytorch.bat  # [win]
    build:
      skip: True  # [py<38]
      string: gpu_cuda{{ cudatoolkit | replace('.', '') }}py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}            # [gpu_variant == "cuda-11"]
      string: gpu_cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}  # [gpu_variant == "cuda-12"]
      string: gpu_mps_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                                # [gpu_variant == "metal"]
      string: cpu_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                                    # [gpu_variant == "cpu"]
      detect_binary_files_with_prefix: False
      entry_points:
        - torchrun = torch.distributed.run:main
      missing_dso_whitelist:
        # For some reason, the .sos built for python 3.11 look for .sos for linking in directories of a different
        # python. This patches over this, although it's not great and the problem should be fixed properly.
        # Note that this issue is sporadic - you may remove this, and the build may work but then break at a later date.
        - "**/ld64.so.*"              # [linux]
        - "**/libc10.so"              # [linux]
        - "**/libshm.so"              # [linux]
        - "**/libtorch.so"            # [linux]
        - "**/libtorch_cpu.so"        # [linux]
        - "**/libtorch_python.so"     # [linux]
        - "**/libiomp5.so"            # [linux]
        - "**/libc10.dylib"           # [osx]
        - "**/libshm.dylib"           # [osx]
        - "**/libtorch.dylib"         # [osx]
        - "**/libtorch_cpu.dylib"     # [osx]
        - "**/libtorch_python.dylib"  # [osx]
        - "**/libiomp5.dylib"         # [osx]
        - "**/libc++.1.dylib"         # [osx]
        - "**/libomp.dylib"           # [osx]
        # conda-build also can't find these on Windows sometimes, for example Python 3.8, in Powershell with cygwin added to
        # the PATH. Error message is "$RPATH/<libname>.dll not found". Maybe RPATH isn't being defined correctly?
        - "**/asmjit.dll"             # [win]
        - "**/c10.dll"                # [win]
        - "**/fbgemm.dll"             # [win]
        - "**/shm.dll"                # [win]
        - "**/torch_cpu.dll"          # [win]
        - "**/torch_python.dll"       # [win]
        # libcuda.so is the cuda driver API library and is a system library.
        - "**/libcuda.so*"            # [(gpu_variant or "").startswith("cuda")]
        # these aren't found sometimes, maybe for the same reason as at the top of this section
        - "**/libtorch_cuda.so"       # [(gpu_variant or "").startswith("cuda")]
        - "**/libc10_cuda.so"         # [(gpu_variant or "").startswith("cuda")]

    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}          # [gpu_variant == "cuda-12"]
        # The cuda compiler function above pulls in a number of other libraries, leading CMake to find CUDAToolkit in the
        # build environment. See https://github.com/Kitware/CMake/blob/master/Modules/Internal/CMakeCUDAFindToolkit.cmake#L135
        # If these two libraries aren't in the build environment they aren't found by PyTorch's build system as a result.
        # Currently cuda libraries variously from the build and host environments are used by the build system (see logs).
        # If this turns out to be a problem, we can address it.
        - libcublas-dev                   # [gpu_variant == "cuda-12"]
        - cuda-nvtx-dev                   # [gpu_variant == "cuda-12"]
        - cmake
        - make                            # [unix]
        - git                             # [unix]
        - patch                           # [not win]
        - m2-patch                        # [win]
        - python ={{PY_VER}}
        - ninja-base
        # This has a strong run_export so we don't need to put it in `host` or `run`
        # We use llvm-openmp for openblas variants on osx.
        - llvm-openmp                     # [osx and not (blas_impl == "mkl")]
        # Keep libprotobuf here so that a compatibile version
        # of protobuf is installed between build and host
        # Unvendoring protobuf doesn't work properly for windows
        # (due to errors in the Pytorch build system)
        - libprotobuf                     # [not win]
        - protobuf                        # [not win]
      host:
        # GPU requirements
        - cudatoolkit {{ cudatoolkit }}*  # [gpu_variant == "cuda-11"]
        - cudnn {{ cudnn }}*              # [(gpu_variant or "").startswith("cuda")]
        - magma 2.7.1                     # [(gpu_variant or "").startswith("cuda")]
        - cuda-version {{ cudatoolkit }}  # [gpu_variant == "cuda-11"]
        - cuda-driver-dev                 # [gpu_variant == "cuda-12"]
        - cuda-cudart-dev                 # [gpu_variant == "cuda-12"]
        - cuda-nvrtc-dev                  # [gpu_variant == "cuda-12"]
        - cuda-nvtx-dev                   # [gpu_variant == "cuda-12"]
        - cuda-nvml-dev                   # [gpu_variant == "cuda-12"]
        - cuda-profiler-api               # [gpu_variant == "cuda-12"]
        - libcublas-dev                   # [gpu_variant == "cuda-12"]
        - libcufft-dev                    # [gpu_variant == "cuda-12"]
        - libcurand-dev                   # [gpu_variant == "cuda-12"]
        - libcusolver-dev                 # [gpu_variant == "cuda-12"]
        - libcusparse-dev                 # [gpu_variant == "cuda-12"]
        #nccl?
        # Required for GPU profiler
        - cupti {{ cudatoolkit }}*        # [gpu_variant == "cuda-11"]
        - cuda-cupti                      # [gpu_variant == "cuda-12"]
        # OpenBLAS or MKL
        - mkl-devel {{ mkl }}.*           # [blas_impl == "mkl"]
        - mkl {{ mkl }}.*                 # [blas_impl == "mkl"]
        - openblas {{ openblas }}         # [blas_impl == "openblas"]
        # OpenMP
        # We pull in the same versions of mkl and intel-openmp: intel aligns the versions
        # We use intel-openmp for all mkl variants.
        # For openblas on win and linux, we don't specify any openmp implementation; it comes from the compiler.
        - intel-openmp   {{ mkl }}        # [blas_impl == "mkl"]
        # Other requirements
        - cffi 1.15.1
        - libprotobuf {{ libprotobuf }}   # [not win]
        # on osx, libuv supports torch.distributed support. See build.sh.
        - libuv 1.44.2                    # [win or osx]
        - numpy {{ numpy }}
        - pip                             # Required for in tree builds
        - pkg-config 0.29.2               # [unix]
        - python
        - pyyaml 6.0.1
        - requests 2.31.0
        # CF: PyTorch relies on features that were removed in later versions.
        - setuptools
        - sleef 3.5.1                     # [osx and arm64]
        - typing_extensions
        - wheel
        - pybind11 2.10.4
        - eigen 3.3.7
        - astunparse 1.6.3
      run:
        # OpenBLAS or MKL
        - mkl {{ mkl }}.*                                     # [blas_impl == "mkl"]
        - libopenblas                                         # [blas_impl == "openblas"]
        # OpenMP
        - {{ pin_compatible('intel-openmp') }}                # [blas_impl == "mkl"]
        # GPU requirements
        - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [gpu_variant == "cuda-11"]
        - {{ pin_compatible('cudnn') }}                       # [(gpu_variant or "").startswith("cuda")]
        # Required for GPU profiler
        # Required for GPU profiler
        - {{ pin_compatible('cupti') }}                       # [gpu_variant == "cuda-11"]
        - {{ pin_compatible('cuda-cupti') }}                  # [gpu_variant == "cuda-12"]
        # other requirements
        - cffi
        # CF: needed to load C++ extensions
        - ninja
        - {{ pin_compatible('numpy') }}
        - python
        - typing_extensions >=4.8.0
        # To stop the compiler pulling in an openmp implementation itself
        - _openmp_mutex                                       # [linux]
        - {{ pin_compatible('magma') }}                       # [(gpu_variant or "").startswith("cuda")]
        - filelock
        - jinja2
        - networkx
        - sympy
        - fsspec
        # Required to support torch.compile. This is tested in smoke_test.py, which is required to pass
        - triton {{ '.'.join(version.split('.')[:2]) }}       # [(gpu_variant or "").startswith("cuda")]
        - __cuda >={{ cudatoolkit }}                          # [gpu_variant == "cuda-11"]
        # On macOS, the GPU accelerated backend, MPS, can be used from macOS v12.3. This isn't tightly dependent on the
        # SDK version used.
        - __osx >=12.3                                        # [gpu_variant == "metal"]

    test:
      requires:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - expecttest
        - hypothesis
        - mock  # [linux]
        - pip
        - psutil
        - pytest
        - scipy
        - setuptools
        - six
        - tabulate
        - boto3
        - pytest-rerunfailures
        - pytest-shard
        - pytest-flakefinder
        - pytest-xdist
      imports:
        - torch
      files:
        - smoke_test.py
      source_files:
        - test/
        - tools/
      commands:
        # the smoke test script takes a bunch of env variables, defined below
        - set MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cudatoolkit.split('.')[:2]) }}"              # [(gpu_variant == "cuda-11") and (win)]
        - set MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cuda_compiler_version.split('.')[:2]) }}"    # [(gpu_variant == "cuda-12") and (win)]
        - set MATRIX_GPU_ARCH_TYPE="cuda"                                                       # [(gpu_variant or "").startswith("cuda") and (win)]
        - set MATRIX_GPU_ARCH_VERSION="none"                                                    # [(gpu_variant == "cpu") and (win)]
        - set MATRIX_GPU_ARCH_TYPE="none"                                                       # [(gpu_variant == "cpu") and (win)]
        - set MATRIX_CHANNEL="defaults"                                                         # [win]
        - set MATRIX_STABLE_VERSION={{ version }}                                               # [win]
        - set MATRIX_PACKAGE_TYPE="conda"                                                       # [win]
        - set TARGET_OS="windows"                                                               # [win]
        - export MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cudatoolkit.split('.')[:2]) }}"           # [(gpu_variant == "cuda-11") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cuda_compiler_version.split('.')[:2]) }}" # [(gpu_variant == "cuda-12") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_VERSION="{{ MACOSX_SDK_VERSION }}"                             # [(gpu_variant == "metal")]
        - export MATRIX_GPU_ARCH_TYPE="cuda"                                                    # [(gpu_variant or "").startswith("cuda") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_TYPE="mps"                                                     # [(gpu_variant == "metal")]
        - export MATRIX_GPU_ARCH_VERSION="none"                                                 # [(gpu_variant == "cpu") and (not win)]
        - export MATRIX_GPU_ARCH_TYPE="none"                                                    # [(gpu_variant == "cpu") and (not win)]
        - export MATRIX_CHANNEL="defaults"                                                      # [not win]
        - export MATRIX_STABLE_VERSION="{{ version }}"                                          # [not win]
        - export MATRIX_PACKAGE_TYPE="conda"                                                    # [not win]
        - export TARGET_OS="unix"                                                               # [not win]
        - python ./smoke_test.py --package torchonly
        # We seem to have individual platform-specific test failures or flaky
        # tests, but the majority of tests pass.
        # Note that the `|| true` expression will make the build continue even if the whole script falls over completely
        # (for example, in the case of missing imports). There doesn't seem to be a way of making a script exception return
        # non-zero but failing tests return zero.
        # The inductor tests test the torch.compile backend
        - python ./test/run_test.py --inductor --continue-through-error || true       # [(gpu_variant or "").startswith("cuda") and (linux and x86_64)]
        - python ./test/run_test.py --core --continue-through-error || true
        # Run pip check so as to ensure that all pytorch packages are installed
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
        - pip check
        - python -c "import torch; print(torch.__version__)"
        # We have had issues with openmp .dylibs being doubly loaded in certain cases. These two tests catch those issues
        - python -c "import torch; import numpy"
        - python -c "import numpy; import torch"
        # distributed support is enabled by default on linux; for mac, we enable it manually in build.sh
        - python -c "import torch; assert torch.distributed.is_available()"        # [linux or osx]
        - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [blas_impl == "mkl"]
        - python -c "import torch; assert torch.backends.cuda.is_built()"          # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.cudnn.is_available()"     # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.cuda.is_available()"               # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.cudnn.enabled"            # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.mps.is_built()"           # [(gpu_variant == "metal")]

about:
  home: https://pytorch.org/
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
  description: |
    PyTorch is a Python package that provides two high-level features:
      - Tensor computation (like NumPy) with strong GPU acceleration
      - Deep neural networks built on a tape-based autograd system
    You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.
  dev_url: https://github.com/pytorch/pytorch
  doc_url: https://pytorch.org/docs/

extra:
  recipe-maintainers:
    - tobijk
    - danpetry
