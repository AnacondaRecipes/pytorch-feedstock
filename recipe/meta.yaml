{% set version = "2.7.0" %}
{% set sha256 = "ecca266fa2de4235a9fd5a18a33299a9de55ab3babb87f8c297c1c9ab8d436bd" %}
# Set the RC number to build release candidates. Set to None otherwise
{% set rc = None %}
{% set build = 0 %}

# Keep this in sync with the release
{% set smoke_test_commit = "1eba9b3aa3c43f86f4a2c807ac8e12c4a7767340" %}

# Use a higher build number for the CUDA variant, to ensure that it's
# preferred by conda's solver, and it's preferentially
# installed where the platform supports it.
{% if gpu_variant != "cpu" %}
{% set build = build + 200 %}
{% endif %}

{% if blas_impl == "mkl" %}
{% set build = build + 100 %}
{% endif %}

{% if not (gpu_variant or "").startswith("cuda") %}
{% set cuda_major = 0 %}
{% else %}
{% set cuda_major = environ.get("cuda_compiler_version", "11.8").split(".")[0] | int %}
{% endif %}

package:
  name: libtorch
  version: {{ version }}

source:
{% if rc != None %}
  # - git_url: https://github.com/pytorch/pytorch.git
  #   git_rev: v{{ version.replace(".rc", "-rc") }}
  # we cannot apply patches to submodules when checking out with git_url, because
  # then conda switches the patch-application to use git, which cannot construct
  # a usable ancestor from outside the submodule; the only option then is to
  # pull in the submodules separately.
  - url: https://github.com/pytorch/pytorch/archive/refs/tags/v{{ version }}.tar.gz
    sha256: 04ae0a8babdc9cb9dfc4f8746b2b8aa0f8ed0f9e92835cc4af0bcb01e3969e51
{% else %}
  # The "pytorch-v" tarballs contain submodules; the "pytorch-" ones don't.
  - url: https://github.com/pytorch/pytorch/releases/download/v{{ version }}/pytorch-v{{ version }}.tar.gz
    sha256: {{ sha256 }}
    patches:
      - patches/0001-windows-FindMKL-add-library-suffix.patch                  # [win]
      - patches/0002-swap-openmp-search-precedence.patch                       # [blas_impl == "mkl"]
      - patches/0003-Force-usage-of-python-3-and-error-without-numpy.patch
      # https://github.com/pytorch/pytorch/pull/137084
      - patches/0004-Help-find-numpy.patch
      # https://github.com/pytorch/pytorch/issues/150918 - continue tests on failure due to flaky tests
      - patches/0007-continue-tests-on-failure.patch
      - patches/0008-add-missing-includes.patch
      - patches/0009-use-prefix-include-for-inductor.patch
      - patches/0010-make-ATEN_INCLUDE_DIR-relative-to-TORCH_INSTALL_PREF.patch 
      - patches/0011-remove-DESTINATION-lib-from-CMake-install-TARGETS-di.patch             # [win]
      - patches_submodules/0001-remove-DESTINATION-lib-from-CMake-install-directives.patch  # [win]
      - patches/0014-point-include-paths-to-PREFIX-include.patch
      - patches/0015-point-lib-paths-to-PREFIX-lib.patch
      - patches/0016-Do-not-check-out-nccl-when-not-building-it.patch
{% endif %}
  - url: https://raw.githubusercontent.com/pytorch/pytorch/{{ smoke_test_commit }}/.ci/pytorch/smoke_test/smoke_test.py
    folder: smoke_test
  # The .gitignore is needed in order to run upstreams `setup.py clean`
  - url: https://raw.githubusercontent.com/pytorch/pytorch/refs/tags/v{{ version }}/.gitignore

build:
  number: {{ build }}
  string: gpu_cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [gpu_variant == "cuda-12"]
  string: gpu_mps_h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                                   # [gpu_variant == "metal"]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [gpu_variant == "cpu"]
  detect_binary_files_with_prefix: false
  run_exports:
    - {{ pin_subpackage('libtorch', max_pin='x.x') }}
  ignore_run_exports_from:
    - python *                               # [megabuild]
    - numpy *                                # [megabuild]
    - cross-python_{{ target_platform }}     # [megabuild and build_platform != target_platform]
  ignore_run_exports:
    - python *                               # [megabuild]
    - numpy *                                # [megabuild]
  skip: True  # [py<39]

requirements:
  # Keep this list synchronized (except for python*, numpy*) in outputs
  # We use python to build libtorch as well because it is easier
  build:
    # When you change megabuild_python, change it in build.sh as well
    - python {{ megabuild_python }}          # [megabuild and build_platform != target_platform]
    - python                                 # [not megabuild and build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]
    - numpy  *                               # [megabuild and build_platform != target_platform]
    - numpy                                  # [not megabuild and build_platform != target_platform]
    - {{ stdlib('c') }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                 # [(gpu_variant or "").startswith("cuda")]
    - nvtx-c                                 # [cuda_compiler_version != "None" and build_platform != target_platform]
    {% if cuda_major >= 12 %}
    - cuda-driver-dev                        # [build_platform != target_platform]
    - cuda-cudart-dev                        # [build_platform != target_platform]
    - cuda-nvrtc-dev                         # [build_platform != target_platform]
    - cuda-nvtx-dev                          # [build_platform != target_platform]
    - cuda-nvml-dev                          # [build_platform != target_platform]
    - cuda-profiler-api                      # [build_platform != target_platform]
    - libcublas-dev                          # [build_platform != target_platform]
    - libcufft-dev                           # [build_platform != target_platform]
    - libcurand-dev                          # [build_platform != target_platform]
    - libcusolver-dev                        # [build_platform != target_platform]
    - libcusparse-dev                        # [build_platform != target_platform]
    {% endif %}
    # Dec 2020: it seems that git is broken on windows, so we use m2-git
    - m2-patch  # [win]
    - m2-git    # [win]
    - patch     # [not win]
    - git       # [not win]
    - libgomp        # [linux]
    # This has a strong run_export so we don't need to put it in `host` or `run`
    # We use llvm-openmp for openblas variants on osx.
    - llvm-openmp 17              # [osx and not (blas_impl == "mkl")]
    - libuv     # [win]
    - cmake
    - ninja-base
    - libabseil
    # Keep libprotobuf here so that a compatibile version
    # of protobuf is installed between build and host
    - libprotobuf
    - protobuf
    - make      # [linux]
    # Uncomment to use ccache, see README and build_pytorch.sh
    # - ccache
  host:
    # GPU requirements
    - cudnn 9.1.1.17                  # [(gpu_variant or "").startswith("cuda")]
    - nccl 2.21.5.1                   # [(gpu_variant or "").startswith("cuda")]
    - magma 2.7.1                     # [(gpu_variant or "").startswith("cuda")]
    - cuda-version {{ cuda_compiler_version }}  # [(gpu_variant or "").startswith("cuda")]
    - nvtx-c                          # [(gpu_variant or "").startswith("cuda")]
    {% if cuda_major >= 12 %}
    - cuda-driver-dev
    - cuda-cudart-dev
    - cuda-nvrtc-dev
    - cuda-nvtx-dev
    - cuda-nvml-dev
    - cuda-profiler-api
    - libcublas-dev
    - libcufft-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
    - cuda-cupti
    {% endif %}
    # other requirements
    - python {{megabuild_python}}  # [megabuild]
    - python       # [not megabuild]
    - numpy 2
    - pip
    # Upper bound due to https://github.com/pytorch/pytorch/issues/136541
    - setuptools <=72.1.0
    - wheel
    - pyyaml
    - requests
    - future  # [py<313]
    - six
    - mkl-devel {{ mkl }}           # [blas_impl == "mkl"]
    - openblas-devel {{ openblas }}   # [blas_impl == "openblas"]
    # - libcblas * *_mkl      # [blas_impl == "mkl"]
    # - libcblas              # [blas_impl != "mkl"]
    # - liblapack             # [blas_impl != "mkl"]
    # - libgomp   # [linux]
    # We pull in the same versions of mkl and intel-openmp: intel aligns the versions
    # We use intel-openmp for all mkl variants.
    # For openblas on win and linux, we don't specify any openmp implementation; it comes from the compiler.
    - intel-openmp   {{ mkl }}        # [blas_impl == "mkl"]
    - llvm-openmp 17              # [osx and not (blas_impl == "mkl")]
    - libabseil
    - libprotobuf {{ libprotobuf }}
    - sleef 3.5.1
    - typing
    - libuv
    - pkg-config  # [unix]
    - typing_extensions
    - pybind11 2.12.1
    - eigen 3.3.7
    - astunparse 1.6.3
    - opentelemetry-api
  # satisfy overlinking checks
  run:
    - {{ pin_compatible('intel-openmp') }}   # [blas_impl == "mkl"]
    - libuv  # [win]
    - {{ pin_compatible('magma') }}                       # [(gpu_variant or "").startswith("cuda")]

# these tests are for the libtorch output below, but due to
# a particularity of conda-build, that output is defined in
# the global build stage, including tests
test:
  requires:
    # cmake needs a compiler to run package detection, see
    # https://discourse.cmake.org/t/questions-about-find-package-cli-msvc/6194
    - {{ compiler('cxx') }}
    # for CMake config to find cuda & nvrtc
    - {{ compiler('cuda') }}    # [(gpu_variant or "").startswith("cuda")]
    - cuda-nvrtc-dev            # [(gpu_variant or "").startswith("cuda")]
    - cmake
    - ninja
    - pkg-config
  files:
    - cmake_test/
  commands:
    # libraries
    {% for each_lib in [ 'libc10', 'libshm', 'libtorch', 'libtorch_cpu', 'libtorch_global_deps'] %}
    - test -f $PREFIX/lib/{{ each_lib }}.so     # [linux]
    - test -f $PREFIX/lib/{{ each_lib }}.dylib  # [osx]
    {% endfor %}
    {% for each_lib in ['libc10_cuda', 'libcaffe2_nvrtc', 'libtorch_cuda', 'libtorch_cuda_linalg'] %}
    - test -f $PREFIX/lib/{{ each_lib }}.so     # [linux and (gpu_variant or "").startswith("cuda")]
    {% endfor %}
        # test integrity of CMake metadata
    - cd cmake_test
    - cmake -GNinja -DCMAKE_CXX_STANDARD=17 $CMAKE_ARGS .   # [unix]
    - cmake -GNinja -DCMAKE_CXX_STANDARD=17 %CMAKE_ARGS% .  # [win]

outputs:
  - name: libtorch
    build:
      missing_dso_whitelist:
        # The are dynamically loaded from %SP_DIR%\torch\lib\
        - "**/asmjit.dll"             # [win]
        - "**/c10.dll"                # [win]
        - "**/fbgemm.dll"             # [win]
        - "**/shm.dll"                # [win]
        - "**/torch_cpu.dll"          # [win]
        - "**/torch_python.dll"       # [win]
        - $RPATH/ld64.so.1  # [s390x]
        # libcuda.so is the cuda driver API library and is a system library.
        - "**/libcuda.so*"            # [(gpu_variant or "").startswith("cuda")]
  - name: pytorch
    build:
      string: gpu_cuda{{ cuda_compiler_version | replace('.', '') }}_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [gpu_variant == "cuda-12"]
      string: gpu_mps_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                                   # [gpu_variant == "metal"]
      string: cpu_{{ blas_impl }}_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                     # [gpu_variant == "cpu"]
      entry_points:
        - torchrun = torch.distributed.run:main
      ignore_run_exports:             # [osx]
        - libuv                       # [osx]
      missing_dso_whitelist:
        # The are dynamically loaded from %SP_DIR%\torch\lib\
        - "**/asmjit.dll"             # [win]
        - "**/c10.dll"                # [win]
        - "**/fbgemm.dll"             # [win]
        - "**/shm.dll"                # [win]
        - "**/torch_cpu.dll"          # [win]
        - "**/torch_python.dll"       # [win]
        - $RPATH/ld64.so.1  # [s390x]
      detect_binary_files_with_prefix: false
      run_exports:
        - {{ pin_subpackage('pytorch', max_pin='x.x') }}
        - {{ pin_subpackage('libtorch', max_pin='x.x') }}
      skip: True  # [py<39]

    script: build_pytorch.sh   # [unix]
    script: build_pytorch.bat  # [win]
    requirements:
      build:
        - python                                 # [build_platform != target_platform]
        - cross-python_{{ target_platform }}     # [build_platform != target_platform]
        - numpy                                  # [build_platform != target_platform]
        - {{ stdlib('c') }}
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}                 # [(gpu_variant or "").startswith("cuda")]
        - nvtx-c                                 # [(gpu_variant or "").startswith("cuda") and build_platform != target_platform]
        {% if cuda_major >= 12 %}
        - cuda-driver-dev                        # [build_platform != target_platform]
        - cuda-cudart-dev                        # [build_platform != target_platform]
        - cuda-nvrtc-dev                         # [build_platform != target_platform]
        - cuda-nvtx-dev                          # [build_platform != target_platform]
        - cuda-nvml-dev                          # [build_platform != target_platform]
        - cuda-profiler-api                      # [build_platform != target_platform]
        - libcublas-dev                          # [build_platform != target_platform]
        - libcufft-dev                           # [build_platform != target_platform]
        - libcurand-dev                          # [build_platform != target_platform]
        - libcusolver-dev                        # [build_platform != target_platform]
        - libcusparse-dev                        # [build_platform != target_platform]
        {% endif %}
        # Dec 2020: it seems that git is broken on windows, so we use m2-git
        - patch     # [not win]
        - m2-patch  # [win]
        - git       # [not win]
        - m2-git    # [win]
        - libgomp   # [linux]
        # This has a strong run_export so we don't need to put it in `host` or `run`
        # We use llvm-openmp for openblas variants on osx.
        - llvm-openmp 17              # [osx and not (blas_impl == "mkl")]
        - cmake
        - ninja-base
        # Keep libprotobuf here so that a compatibile version
        # of protobuf is installed between build and host
        - libprotobuf
        - protobuf
        - make      # [linux]
        # Uncomment to use ccache, see README and build_pytorch.sh
        # - ccache
      host:
        # GPU requirements
        - cudnn 9.1.1.17                  # [(gpu_variant or "").startswith("cuda")]
        - nccl 2.21.5.1                   # [(gpu_variant or "").startswith("cuda")]
        - magma 2.7.1                     # [(gpu_variant or "").startswith("cuda")]
        - nvtx-c                          # [(gpu_variant or "").startswith("cuda")]
        {% if cuda_major >= 12 %}
        - cuda-driver-dev
        - cuda-cudart-dev
        - cuda-nvrtc-dev
        - cuda-nvtx-dev
        - cuda-nvml-dev
        - cuda-profiler-api
        - libcublas-dev
        - libcufft-dev
        - libcurand-dev
        - libcusolver-dev
        - libcusparse-dev
        - cuda-cupti
        {% endif %}
        # other requirements
        - python
        - numpy 2
        - pip
        - setuptools <=72.1.0
        - wheel
        - pyyaml
        - requests
        - future  # [py<313]
        - six
        - mkl-devel {{ mkl }}           # [blas_impl == "mkl"]
        - openblas-devel {{ openblas }}   # [blas_impl == "openblas"]
        # - libcblas * *_mkl      # [blas_impl == "mkl"]
        # - libcblas              # [blas_impl != "mkl"]
        # - liblapack             # [blas_impl != "mkl"]
        # - libgomp   # [linux]
        # We pull in the same versions of mkl and intel-openmp: intel aligns the versions
        # We use intel-openmp for all mkl variants.
        # For openblas on win and linux, we don't specify any openmp implementation; it comes from the compiler.
        - intel-openmp   {{ mkl }}        # [blas_impl == "mkl"]
        - llvm-openmp 17              # [osx and not (blas_impl == "mkl")]
        - libabseil
        - libprotobuf {{ libprotobuf }}
        - sleef 3.5.1
        - typing
        - libuv
        - pkg-config  # [unix]
        - typing_extensions
        - {{ pin_subpackage('libtorch', exact=True) }}
        - pybind11 2.12.1
        - eigen 3.3.7
        - astunparse 1.6.3
        - opentelemetry-api
      run:
        - {{ pin_compatible('intel-openmp') }}   # [blas_impl == "mkl"]
        - llvm-openmp                            # [osx and not (blas_impl == "mkl")]
        # GPU requirements without run_exports
        - {{ pin_compatible('cudnn') }}          # [(gpu_variant or "").startswith("cuda")]
        # Required for GPU profiler
        - {{ pin_compatible('cuda-cupti') }}                  # [gpu_variant == "cuda-12"]
        # other requirements
        - python
        # from https://numpy.org/neps/nep-0029-deprecation_policy.html
        - numpy >=1.24.0,<3.0.0
        - typing_extensions
        # To stop the compiler pulling in an openmp implementation itself (although I'm not sure this mutex works anyway)
        - _openmp_mutex                                       # [linux]
        - {{ pin_compatible('magma') }}                       # [(gpu_variant or "").startswith("cuda")]
        # skip to 1.13.3 to avoid test failures on Windows and mac - align with 2.7.0 release requirement
        # https://github.com/pytorch/pytorch/pull/133235
        - sympy>=1.13.3
        - filelock
        - jinja2
        - networkx
        - nomkl                 # [blas_impl != "mkl"]
        - fsspec
        # Required to support torch.compile. This is tested in smoke_test.py, which is required to pass
        - triton 3.3.0       # [(gpu_variant or "").startswith("cuda") and (linux and x86_64)]
        # avoid that people without GPUs needlessly download ~0.5-1GB
        # The CUDA version constraint is handled in cuda-version as a run_constrained.
        # However, that doesn't enforce that the package requires a GPU; that needs to be done here.
        - __cuda  # [(gpu_variant or "").startswith("cuda")]
        # On macOS, the GPU accelerated backend, MPS, can be used from macOS v12.3. This isn't tightly dependent on the
        # SDK version used.
        - __osx >=12.3                                               # [gpu_variant == "metal"]
        - libtorch {{ version }}
        - setuptools
        - opentelemetry-api
      run_constrained:
        # current intel-openmp builds are incompatible with llvm-openmp on osx-64
        - llvm-openmp <0a0                # [(blas_impl == "mkl") and (osx and x86_64)]  

    test:
      requires:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - boto3
        - hypothesis
        - pytest
        - tabulate
        - pydot
        - mock  # [linux]
        - pip
        - expecttest
        # unittest-xml-reporting provides xmlrunner
        - unittest-xml-reporting
        - pytest-rerunfailures
        - pytest-flakefinder
        - pytest-xdist
        # Needed for test_autograd.py
        - pybind11
        # the inductor "test_aoti_eager..." tests require objcopy
        - binutils  # [linux]
      imports:
        - torch
      source_files:
        - test
        # tools/ is needed to optimise test run
        # as of pytorch=2.0.0, there is a bug when trying to run tests without the tools
        - tools
        - smoke_test
        # See files needed: https://github.com/pytorch/pytorch/blob/main/test/test_ops.py#L271-L274
        - aten/src/ATen/native
      commands:
        # the smoke test script takes a bunch of env variables, defined below
        - set MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cudatoolkit.split('.')[:2]) }}"              # [(gpu_variant == "cuda-11") and (win)]
        - set MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cuda_compiler_version.split('.')[:2]) }}"    # [(gpu_variant == "cuda-12") and (win)]
        - set MATRIX_GPU_ARCH_TYPE="cuda"                                                       # [(gpu_variant or "").startswith("cuda") and (win)]
        - set MATRIX_GPU_ARCH_VERSION="none"                                                    # [(gpu_variant == "cpu") and (win)]
        - set MATRIX_GPU_ARCH_TYPE="none"                                                       # [(gpu_variant == "cpu") and (win)]
        - set MATRIX_CHANNEL="defaults"                                                         # [win]
        - set MATRIX_STABLE_VERSION={{ version }}                                               # [win]
        - set MATRIX_PACKAGE_TYPE="conda"                                                       # [win]
        - set TARGET_OS="windows"                                                               # [win]
        - set OMP_NUM_THREADS=4                                                                 # [win]
        - export MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cudatoolkit.split('.')[:2]) }}"           # [(gpu_variant == "cuda-11") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_VERSION="{{ '.'.join(cuda_compiler_version.split('.')[:2]) }}" # [(gpu_variant == "cuda-12") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_VERSION="{{ MACOSX_SDK_VERSION }}"                             # [(gpu_variant == "metal")]
        - export MATRIX_GPU_ARCH_TYPE="cuda"                                                    # [(gpu_variant or "").startswith("cuda") and (linux and x86_64)]
        - export MATRIX_GPU_ARCH_TYPE="mps"                                                     # [(gpu_variant == "metal")]
        - export MATRIX_GPU_ARCH_VERSION="none"                                                 # [(gpu_variant == "cpu") and (not win)]
        - export MATRIX_GPU_ARCH_TYPE="none"                                                    # [(gpu_variant == "cpu") and (not win)]
        - export MATRIX_CHANNEL="defaults"                                                      # [not win]
        - export MATRIX_STABLE_VERSION="{{ version }}"                                          # [not win]
        - export MATRIX_PACKAGE_TYPE="conda"                                                    # [not win]
        - export TARGET_OS="linux"                                                              # [linux]
        - export TARGET_OS="macos-arm64"                                                        # [(osx and arm64)]
        - export TARGET_OS="macos-x86_64"                                                       # [(osx and x86_64)]
        - export OMP_NUM_THREADS=4                                                              # [not win]
        - python ./smoke_test/smoke_test.py --package torchonly
        # We seem to have individual platform-specific test failures or flaky
        # tests, but the majority of tests pass.
        # Note that the `|| true` expression will make the build continue even if the whole script falls over completely
        # (for example, in the case of missing imports). There doesn't seem to be a way of making a script exception return
        # non-zero but failing tests return zero.
        # ------------------------------------------------------------------------------------------------
        # Exclude complex tests that are known to be flaky for -k "not (complex and (linalg_vecdot or dot or vdot))"
        # https://github.com/pytorch/pytorch/issues/150918
        - python ./test/run_test.py --core --continue-through-error -k "not (complex and (linalg_vecdot or dot or vdot))" || true
        # The inductor tests test the torch.compile backend. Using the options below avoids running distributed tests,
        # which would be run if we used the --inductor option. (Distributed tests would only be correctly run on a multi-gpu test platform,
        # which we don't have.)
        # torch.compile isn't supported on python 3.12 yet
        - python test/run_test.py -i inductor/test_torchinductor.py --continue-through-error || true                 # [(gpu_variant or "").startswith("cuda") and (linux and x86_64)]
        - python ./test/run_test.py --mps --continue-through-error || true                                           # [(gpu_variant == "metal")]
        # Run pip check so as to ensure that all pytorch packages are installed
        # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
        - pip check
        - python -c "import torch; print(torch.__version__)"
        # We have had issues with openmp .dylibs being doubly loaded in certain cases. These two tests catch those issues
        - python -c "import torch; import numpy"
        - python -c "import numpy; import torch"
        # distributed support is enabled by default on linux; for mac, we enable it manually in build.sh
        - python -c "import torch; assert torch.distributed.is_available()"        # [linux or osx]
        # mkldnn is set in build.sh for linux/non-cuda variants
        - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [linux and not (gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.cuda.is_built()"          # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.cudnn.is_available()"     # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.cuda.is_available()"               # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.cudnn.enabled"            # [(gpu_variant or "").startswith("cuda")]
        - python -c "import torch; assert torch.backends.mps.is_built()"           # [(gpu_variant == "metal")]
        - python -c "import torch; torch.tensor(1).to('cpu').numpy(); print('numpy support enabled!!!')"
        - test -f $PREFIX/lib/libtorch_python${SHLIB_EXT}     # [unix]

  - name: pytorch-{{ "cpu" if gpu_variant == "cpu" else "gpu" }}
    requirements:
      run:
        - pytorch ={{ version }}={{ "cpu" if gpu_variant == "cpu" else "gpu" }}* # NB pinning exact=True will also pin to the python version
    test:
      imports:
        - torch

about:
  home: https://pytorch.org/
  dev_url: https://github.com/pytorch/pytorch
  license: BSD-3-Clause
  license_family: BSD
  license_file:
    - LICENSE
    - NOTICE
    - third_party/pybind11/LICENSE
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
  description: |
    PyTorch is a Python package that provides two high-level features:
      - Tensor computation (like NumPy) with strong GPU acceleration
      - Deep neural networks built on a tape-based autograd system
    You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.
  doc_url: https://pytorch.org/docs/

extra:
  skip-lints:
    - missing_tests
  recipe-maintainers:
    - hmaarrfk
    - sodre
    - benjaminrwilson
    - Tobias-Fischer
    - beckermr
    - danpetry
