{% set version = "2.0.1" %}
{% set sha256 = "9c564ca440265c69400ef5fdd48bf15e28af5aa4bed84c95efaad960a6699998" %}

package:
  name: pytorch
  version: {{ version }}

source:
  # The "pytorch-v" tarballs contain submodules; the "pytorch-" ones don't.
  url: https://github.com/pytorch/pytorch/releases/download/v{{ version }}/pytorch-v{{ version }}.tar.gz
  sha256: {{ sha256 }}
  patches:
    # fixes linker error on linux platforms, see https://github.com/pytorch/pytorch/issues/90448
    - patches/fix_kDefaultTimeout.patch
    - patches/0001-windows-FindMKL-add-library-suffix.patch  # [win]

build:
  number: 0
  skip: True  # [py<38]
  # Dropping ppc because of various build issues
  skip: True  # [(linux and ppc64le)]
  # Only building for win-64 CPU in this PR
  skip: True  # [not (win-64)]
  string: gpu_cuda{{ cudatoolkit | replace('.', '') }}py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}  # [(pytorch_variant == "gpu") and (linux and x86_64)]
  string: gpu_mps_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                      # [(pytorch_variant == "gpu") and (osx and arm64)]
  string: cpu_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                          # [pytorch_variant == "cpu"]
  detect_binary_files_with_prefix: False
  entry_points:
    - torchrun = torch.distributed.run:main
  missing_dso_whitelist:
    - "**/libcuda.so*"                # [(pytorch_variant == "gpu") and (linux and x86_64)]
  track_features:
    - pytorch-cpu                     # [pytorch_variant == "cpu"]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - cmake
    - make                            # [unix]
    - git                             # [unix]
    - patch                           # [not win]
    - m2-patch                        # [win]
    - python ={{PY_VER}}
    - ninja-base
    # This has a strong run_export so we don't need to put it in `host` or `run`
    # We use llvm-openmp for openblas variants on osx.
    - llvm-openmp                     # [osx and not (blas_impl == "mkl")]
    # Keep libprotobuf here so that a compatibile version
    # of protobuf is installed between build and host
    # Unvendoring protobuf doesn't work properly for windows
    # (due to errors in the Pytorch build system)
    - libprotobuf                     # [not win]
    - protobuf                        # [not win]
  host:
    # GPU requirements
    - cudatoolkit {{ cudatoolkit }}*  # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - cudnn {{ cudnn }}*              # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - magma 2.7.1                     # [(pytorch_variant == "gpu") and (linux and x86_64)]
    # Required for GPU profiler
    - cupti 11.8.0                    # [(pytorch_variant == "gpu") and (linux and x86_64)]
    # OpenBLAS or MKL
    - mkl-devel {{ mkl }}.*           # [blas_impl == "mkl"]
    - mkl {{ mkl }}.*                 # [blas_impl == "mkl"]
    - openblas {{ openblas }}         # [blas_impl == "openblas"]
    # OpenMP
    # We pull in the same versions of mkl and intel-openmp: intel aligns the versions
    # We use intel-openmp for all mkl variants.
    # For openblas on win and linux, we don't specify any openmp implementation; it comes from the compiler.
    - intel-openmp   {{ mkl }}        # [blas_impl == "mkl"]
    # Other requirements
    - cffi 1.15.1
    - libprotobuf {{ libprotobuf }}   # [not win]
    # on osx, libuv supports torch.distributed support. See build.sh.
    - libuv 1.44.2                    # [win or osx]
    - numpy {{ numpy }}
    - pip                             # Required for in tree builds
    - pkg-config 0.29.2               # [unix]
    - python
    - pyyaml 6.0
    - requests 2.28.1
    # CF: PyTorch relies on features that were removed in later versions.
    - setuptools
    - sleef 3.5.1                     # [osx and arm64]
    - typing-extensions 4.4.0
    - wheel
    - pybind11 2.10.1
    - eigen 3.3.7
    - astunparse 1.6.3
  run:
    # OpenBLAS or MKL
    - mkl {{ mkl }}.*                 # [blas_impl == "mkl"]
    - libopenblas                     # [blas_impl == "openblas"]
    # OpenMP
    - {{ pin_compatible('intel-openmp') }}   # [blas_impl == "mkl"]
    # GPU requirements
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - {{ pin_compatible('cudnn') }}                       # [(pytorch_variant == "gpu") and (linux and x86_64)]
    # Required for GPU profiler
    - cupti                           # [(pytorch_variant == "gpu") and (linux and x86_64)]
    # other requirements
    - cffi
    # CF: needed to load C++ extensions
    - ninja
    - {{ pin_compatible('numpy') }}
    - python
    - typing-extensions
    # To stop the compiler pulling in an openmp implementation itself
    - _openmp_mutex                   # [linux]
    - magma                           # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - filelock
    - jinja2
    - networkx
    - sympy
    - __cuda >={{ cudatoolkit }}      # [(pytorch_variant == "gpu") and (linux and x86_64)]

test:
  requires:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - expecttest
    - hypothesis
    - mock  # [linux]
    - pip
    - psutil
    - pytest
    - scipy
    - setuptools
    - six
    - tabulate
  imports:
    - torch
  source_files:
    - test/
  commands:
    # We seem to have individual platform-specific test failures or flaky
    # tests, but the majority of tests pass.
    - set CONTINUE_THROUGH_ERROR=1     # [win]
    - export CONTINUE_THROUGH_ERROR=1  # [not win]
    - python ./test/run_test.py --core || true
    # Run pip check so as to ensure that all pytorch packages are installed
    # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
    - pip check
    - python -c "import torch; print(torch.__version__)"
    # We have had issues with openmp .dylibs being doubly loaded in certain cases. These two tests catch those issues
    - python -c "import torch; import numpy"
    - python -c "import numpy; import torch"
    # distributed support is enabled by default on linux; for mac, we enable it manually in build.sh
    - python -c "import torch; assert torch.distributed.is_available()"        # [linux or osx]
    - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [x86 and cuda_compiler_version == "None"]
    - python -c "import torch; assert torch.backends.cuda.is_built()"          # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - python -c "import torch; assert torch.backends.cudnn.is_available()"     # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - python -c "import torch; assert torch.cuda.is_available()"               # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - python -c "import torch; assert torch.backends.cudnn.enabled"            # [(pytorch_variant == "gpu") and (linux and x86_64)]
    - python -c "import torch; assert torch.backends.mps.is_built()"           # [(pytorch_variant == "gpu") and (osx and arm64)]

about:
  home: https://pytorch.org/
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
  description: |
    PyTorch is a Python package that provides two high-level features:
      - Tensor computation (like NumPy) with strong GPU acceleration
      - Deep neural networks built on a tape-based autograd system
    You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.
  dev_url: https://github.com/pytorch/pytorch
  doc_url: https://pytorch.org/docs/

extra:
  recipe-maintainers:
    - tobijk
