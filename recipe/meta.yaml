{% set version = "2.0.0" %}
{% set sha256 = "cecc38b6d4256b810336edfc6119d7a57b701fdf1ba43c50001f31e2724fd8e2" %}

package:
  name: pytorch
  version: {{ version }}

source:
  # The "pytorch-v" tarballs contain submodules; the "pytorch-" ones don't.
  url: https://github.com/pytorch/pytorch/releases/download/v{{ version }}/pytorch-v{{ version }}.tar.gz
  sha256: {{ sha256 }}
  patches:
    # fixes compilation error on osx-64 due to DISPATCH_APPLY_AUTO #define being not found
    - patches/fix_dispatch_apply_auto.patch

build:
  number: 0
  skip: True  # [py<37]
  # ppc requires gcc v8 (see cbc.yaml). Python 3.11 is incompatible with anything built with gcc<11 because of runtime conflicts.
  skip: True  # [(linux and ppc64le) and py==311]
  # TODO: note that at the moment we only use CUDA backend, e.g. would be different for mac
  string: gpu_cuda{{ cudatoolkit | replace('.', '') }}py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}  # [pytorch_variant == "gpu"]
  string: cpu_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                          # [pytorch_variant == "cpu"]
  detect_binary_files_with_prefix: False

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - cmake
    - make                            # [unix]
    - git
    - patch                           # [not win]
    - m2-patch                        # [win]
    - python ={{PY_VER}}
    - ninja
    # This has a strong run_export so we don't need to put it in `host` or `run`
    # We use llvm-openmp for openblas variants on osx.
    - llvm-openmp                     # [osx and not (blas_impl == "mkl")]
    # Keep libprotobuf here so that a compatibile version
    # of protobuf is installed between build and host
    # Unvendoring protobuf doesn't work properly for windows
    # (due to errors in the Pytorch build system)
    - libprotobuf                     # [not win]
    - protobuf                        # [not win]
  host:
    # GPU requirements
    - cudatoolkit {{ cudatoolkit }}*  # [pytorch_variant == "gpu"]
    - cudnn {{ cudnn }}*              # [pytorch_variant == "gpu"]
    - magma 2.7.0                     # [pytorch_variant == "gpu"]
    # Required for GPU profiler
    - cupti 11.3.1                    # [pytorch_variant == "gpu"]
    # OpenBLAS or MKL
    - mkl-devel {{ mkl }}.*           # [blas_impl == "mkl"]
    - mkl {{ mkl }}.*                 # [blas_impl == "mkl"]
    - openblas {{ openblas }}                 # [blas_impl == "openblas"]
    # OpenMP
    # We pull in the same versions of mkl and intel-openmp: intel aligns the versions
    # We use intel-openmp for all mkl variants.
    # For openblas on win and linux, we don't specify any openmp implementation; it comes from the compiler.
    - intel-openmp   {{ mkl }}        # [blas_impl == "mkl"]
    # Other requirements
    - cffi 1.15.1
    - future 0.18.2
    - libprotobuf {{ libprotobuf }}       # [not win]
    # on osx, libuv supports torch.distributed support. See build.sh.
    - libuv 1.44.2                    # [win or osx]
    # Pinnings as per pytorch/.circleci/docker/common/install_conda.sh,
    # except for python 3.8 because we don't have the numpy version required
    - numpy 1.23          # [py==311]
    - numpy >=1.21,<1.22  # [py==310]
    - numpy >=1.19,<1.20  # [py<=39]
    - pip                               # Required for in tree builds
    - pkg-config 0.29.2                 # [unix]
    - python
    - pyyaml 6.0
    - requests 2.28.1
    # CF: PyTorch relies on features that were removed in later versions.
    - setuptools
    - sleef 3.5.1                       # [osx and arm64]
    - typing-extensions 4.4.0
    - wheel
    - pybind11 2.10.1
    - eigen 3.3.7
  run:
    # OpenBLAS or MKL
    - mkl {{ mkl }}.*                 # [blas_impl == "mkl"]
    - libopenblas                     # [blas_impl == "openblas"]
    # OpenMP
    - {{ pin_compatible('intel-openmp') }}   # [blas_impl == "mkl"]
    # GPU requirements
    - {{ pin_compatible('cudatoolkit', max_pin='x.x') }}  # [pytorch_variant == "gpu"]
    - {{ pin_compatible('cudnn') }}                       # [pytorch_variant == "gpu"]
    # Required for GPU profiler
    - cupti                           # [pytorch_variant == "gpu"]
    # other requirements
    - cffi
    # CF: pip check may fail if future is not installed
    - future
    # CF: needed to load C++ extensions
    - ninja
    - numpy >=1.23,<2  # [py==311]
    - numpy >=1.21,<2  # [py==310]
    - numpy >=1.19,<2  # [py<=39]
    - python
    - typing-extensions
    # To stop the compiler pulling in an openmp implementation itself
    - _openmp_mutex                   # [linux]
    - pyyaml
    - magma                           # [pytorch_variant == "gpu"]
    - filelock
    - jinja2
    - networkx
    - sympy

test:
  requires:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - expecttest
    - hypothesis
    - mock  # [linux]
    - pip
    - psutil
    - pytest
    - scipy
    - setuptools
    - six
    - tabulate
  imports:
    - torch
  source_files:
    - test/
  commands:
    # We seem to have individual platform-specific test failures or flaky
    # tests, but the majority of tests passes.
    - set CONTINUE_THROUGH_ERROR=1     # [win]
    - export CONTINUE_THROUGH_ERROR=1  # [not win]
    - python ./test/run_test.py --core || true
    # Run pip check so as to ensure that all pytorch packages are installed
    # https://github.com/conda-forge/pytorch-cpu-feedstock/issues/24
    - pip check
    - python -c "import torch; print(torch.__version__)"
    # We have had issues with openmp .dylibs being doubly loaded in certain cases. These two tests catch those issues
    - python -c "import torch; import numpy"
    - python -c "import numpy; import torch"
    # distributed support is enabled by default on linux; for mac, we enable it manually in build.sh
    - python -c "import torch; assert torch.distributed.is_available()"        # [linux or osx]
    - python -c "import torch; assert torch.backends.mkldnn.m.is_available()"  # [x86 and cuda_compiler_version == "None"]
    - python -c "import torch; assert torch.backends.cuda.is_built()"          # [pytorch_variant == "gpu"]
    - python -c "import torch; assert torch.backends.cudnn.is_available()"     # [pytorch_variant == "gpu"]
    - python -c "import torch; assert torch.cuda.is_available()"               # [pytorch_variant == "gpu"]
    - python -c "import torch; assert torch.backends.cudnn.enabled"            # [pytorch_variant == "gpu"]

about:
  home: https://pytorch.org/
  license: BSD-3-Clause
  license_family: BSD
  license_file: LICENSE
  summary: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.
  description: |
    PyTorch is a Python package that provides two high-level features:
      - Tensor computation (like NumPy) with strong GPU acceleration
      - Deep neural networks built on a tape-based autograd system
    You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.
  dev_url: https://github.com/pytorch/pytorch
  doc_url: https://pytorch.org/docs/1.10/index.html

extra:
  recipe-maintainers:
    - tobijk
